{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "777519ec-d99f-4e36-ba2d-6c6b615398d9",
   "metadata": {},
   "source": [
    "## AssQ- DL- CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274c2cbb-ea8c-40a8-9a7f-9a51a707dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d0700-a912-42d2-b737-d5dc775eecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Desccire the pucpose and renejits oj pooling in CNNp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e82d9d-4f2b-4241-9ec3-f4bc76da1f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pooling in Convolutional Neural Networks (CNNs) serves a crucial role in enhancing feature extraction and \n",
    "managing computational complexity. Its primary purpose is to downsample the spatial dimensions of feature \n",
    "maps while retaining important information, thus enabling the network to recognize features regardless of \n",
    "their exact position within an image.\n",
    "\n",
    "Pooling offers several benefits and outcomes:\n",
    "\n",
    "Dimension Reduction: As CNNs process deeper layers, the spatial dimensions of feature maps decrease. Pooling\n",
    "helps manage this reduction by aggregating nearby pixels and extracting their dominant features. This controlled \n",
    "reduction curbs the risk of overfitting and makes the network less sensitive to small spatial variations.\n",
    "\n",
    "Translation Invariance: Pooling ensures that the network recognizes features irrespective of their precise location\n",
    "in the input. This property aids in handling slight translations or shifts in the input data, enhancing the network's\n",
    "ability to generalize.\n",
    "\n",
    "Feature Selection: By retaining the most prominent features through max pooling (or their average through average pooling),\n",
    "the network concentrates on capturing the most relevant information. This leads to a more compact and expressive \n",
    "representation of the input data.\n",
    "\n",
    "Computational Efficiency: As CNNs grow deeper, their computational demands increase. Pooling reduces the number of \n",
    "parameters and computations required in subsequent layers, thereby making the network more efficient and feasible \n",
    "for real-world applications.\n",
    "\n",
    "Noise Reduction: Pooling can mitigate the effects of minor noise in the data. By aggregating neighboring pixels,\n",
    "it diminishes the impact of isolated noisy pixels and emphasizes the stronger, more consistent features.\n",
    "\n",
    "However, pooling also has limitations. Excessive pooling can lead to a loss of fine-grained details, which might\n",
    "be crucial for tasks requiring precise localization. Additionally, as CNNs are designed to automatically learn feature\n",
    "hierarchies, it's not always evident how much pooling to apply at different layers, and aggressive pooling might result\n",
    "in a loss of useful information.\n",
    "\n",
    "In conclusion, pooling in CNNs is pivotal for managing spatial dimensions, extracting important features, and achieving\n",
    "translation invariance. While it enhances the network's efficiency and robustness, careful consideration is needed to \n",
    "strike the right balance between downsampling and retaining essential details for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c9b23-9e9a-4881-b463-19a343966194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883681d8-f543-47b4-b9ba-79625c44b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "2 Explain the diffecence retween Min pooling and Max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63103ca-b751-49e2-978e-2d1e0f9c858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Max pooling and min pooling are two different types of pooling operations used in Convolutional Neural Networks\n",
    "(CNNs) for downsampling feature maps. The key distinction between them lies in how they select values from the\n",
    "pooling window:\n",
    "\n",
    "Max Pooling:\n",
    "Max pooling involves sliding a window over the input feature map and selecting the maximum value within that window.\n",
    "This operation aims to retain the most significant feature present in that region. Max pooling is effective in capturing\n",
    "dominant features and discarding less relevant information. It helps to achieve translation invariance by ensuring \n",
    "that the network identifies key features regardless of their exact location in the input.\n",
    "\n",
    "Min Pooling:\n",
    "Min pooling is similar to max pooling but instead selects the minimum value within the pooling window. While it's\n",
    "less commonly used than max pooling, min pooling has the potential to emphasize the presence of minimal features or\n",
    "low-intensity regions in the input. It might be useful in scenarios where identifying the smallest feature is more \n",
    "important, although this is a relatively rare use case.\n",
    "\n",
    "In practice, max pooling is much more prevalent due to its ability to capture the most salient features and its role\n",
    "in managing spatial dimensions effectively. Min pooling is less common, as the majority of image analysis tasks focus \n",
    "on identifying dominant patterns rather than minimal ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e177656c-473f-42c9-b74a-6520b33e2b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9ec3e-622f-47e0-a280-ff82539b346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "3-Discuss the concept of padding in CNN and its significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9676d1a-b2d3-4486-b045-4354780ccbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Padding in Convolutional Neural Networks (CNNs) is the process of adding extra pixels around the borders of\n",
    "an input image or feature map before applying convolutional operations. Padding introduces artificial information,\n",
    "usually in the form of zeros, to the input data. The primary purpose of padding is to control the spatial dimensions\n",
    "of the output feature maps and to influence the behavior of convolutions near the borders of the input.\n",
    "\n",
    "There are two main types of padding:\n",
    "\n",
    "Valid (No Padding): In this case, no padding is added to the input, and the convolutional filters only slide over the\n",
    "regions where they entirely overlap with the input. This results in smaller output feature maps compared to the input.\n",
    "\n",
    "Same (Zero Padding): Here, padding is added to ensure that the convolutional filters cover the input entirely, even at\n",
    "the borders. The amount of padding is determined by the filter size and is often chosen such that the output feature map \n",
    "has the same spatial dimensions as the input. This padding helps in preserving spatial information and preventing the \n",
    "shrinking of feature maps.\n",
    "\n",
    "The significance of padding in CNNs includes:\n",
    "\n",
    "Preserving Spatial Dimensions: Padding, especially \"same\" padding, ensures that the spatial dimensions of the input and \n",
    "output feature maps remain the same. This is crucial for maintaining fine-grained spatial information throughout the \n",
    "network's layers.\n",
    "\n",
    "Preventing Border Information Loss: Without padding, the edges of the input data undergo fewer convolution operations, \n",
    "leading to a gradual loss of information near the borders. Padding addresses this issue by allowing the convolutional \n",
    "filters to process edge information effectively.\n",
    "\n",
    "Mitigating Overfitting: Padding can help in mitigating overfitting by enabling the network to learn from border regions\n",
    "and reducing the potential for excessive reduction in feature map dimensions.\n",
    "\n",
    "Stabilizing Learning: Zero-padding ensures that the central pixels of the input contribute more significantly to the output,\n",
    "creating a more stable learning process.\n",
    "\n",
    "Handling Larger Receptive Fields: Padding facilitates the use of larger convolutional filters without causing the feature \n",
    "map to shrink excessively, enabling the network to capture broader context.\n",
    "\n",
    "In summary, padding in CNNs plays a vital role in maintaining spatial information, reducing border-related information loss,\n",
    "enhancing network stability, and accommodating different filter sizes. It's a crucial tool in designing effective \n",
    "convolutional layers and ensuring that the network can effectively process inputs of varying sizes while preserving \n",
    "their spatial context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c077f3fb-f13f-43b3-b46c-5032c4ad29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93185e6-60cd-46ce-b125-f59b9e2bd6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "4- Compare and contcast zeco-padding and valid-padding in terms of their effects on the output \n",
    "feature Map size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de6f9be-4434-4641-8bbe-a6e7b5938ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zero-padding and valid-padding are two different techniques used to control the size of the output\n",
    "feature maps in Convolutional Neural Networks (CNNs). They have contrasting effects on the spatial \n",
    "dimensions of the output feature maps.\n",
    "\n",
    "Zero-padding:\n",
    "Zero-padding involves adding extra rows and columns of zeros around the borders of the input feature\n",
    "map before performing convolution. The amount of padding is determined by the size of the convolutional filters\n",
    "and the desired output size. Zero-padding is often used to preserve the spatial dimensions of the input feature\n",
    "map in the output feature map.\n",
    "\n",
    "When using zero-padding, the output feature map's size remains the same as the input size (or very close to it,\n",
    "depending on the filter size). This padding ensures that convolutional operations are applied uniformly across \n",
    "the entire input, preventing loss of information at the borders. Zero-padding is particularly useful when maintaining \n",
    "spatial information, such as in image segmentation tasks, where precise spatial localization is crucial. However, \n",
    "it does increase computational complexity, as it introduces additional calculations involving the padded zeros.\n",
    "\n",
    "Valid-padding:\n",
    "Valid-padding, also known as \"no-padding,\" involves applying convolutional filters to the input feature map without\n",
    "adding any padding. As a result, the convolutional operations are only performed where the filters can fully overlap\n",
    "with the input. This leads to a reduction in the output feature map's size compared to the input size.\n",
    "\n",
    "With valid-padding, the output feature map's size is smaller than the input size, as convolutional operations near the\n",
    "borders are limited by the filter size. This can result in a gradual loss of spatial information as the network processes\n",
    "deeper layers, as the effective receptive field decreases. However, valid-padding is computationally more efficient than \n",
    "zero-padding, as it requires fewer calculations due to the absence of padded regions.\n",
    "\n",
    "In summary, zero-padding preserves the spatial dimensions of the output feature map by introducing additional pixels\n",
    "around the borders, ensuring that convolutional operations cover the entire input. Valid-padding reduces the output \n",
    "feature map's size by performing convolutions without padding, which can lead to a loss of spatial information near \n",
    "the borders. The choice between these padding methods depends on the task's requirements, the desired balance between\n",
    "spatial preservation and computational efficiency, and the nature of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d16b9-8355-4c39-9d4a-3368410b756b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd61cf-b9cf-4649-aac1-0be2805ce69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC: Exploring LeNet\n",
    "\n",
    "\n",
    "1. Provide a brief overview of LeNet-5 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c868ef-51f4-46b2-b67c-6d0997d66fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet-5 is a pioneering convolutional neural network (CNN) architecture developed by Yann LeCun,\n",
    "Leon Bottou, Yoshua Bengio, and Patrick Haffner in the early 1990s. It was one of the first successful\n",
    "CNN models and played a crucial role in popularizing deep learning for image recognition tasks. LeNet-5\n",
    "was specifically designed for handwritten digit recognition, but its principles laid the foundation for\n",
    "modern CNN architectures used in a wide range of computer vision applications.\n",
    "\n",
    "The LeNet-5 architecture consists of seven layers, including three convolutional layers, followed by two \n",
    "fully connected layers, and finally a softmax activation layer for classification:\n",
    "\n",
    "Input Layer: LeNet-5 accepts grayscale images of size 32x32 pixels as input.\n",
    "\n",
    "C1 - Convolutional Layer: The first convolutional layer applies six filters (5x5 kernels) to the input, \n",
    "generating feature maps with dimensions 28x28x6. It employs a sigmoid activation function.\n",
    "\n",
    "S2 - Subsampling (Pooling) Layer: Subsampling is performed using average pooling with 2x2 windows and a\n",
    "stride of 2. This reduces the dimensions to 14x14x6.\n",
    "\n",
    "C3 - Convolutional Layer: This layer introduces a second convolutional stage with sixteen 5x5 filters, \n",
    "producing feature maps sized 10x10x16. The sigmoid activation function is used.\n",
    "\n",
    "S4 - Subsampling (Pooling) Layer: Similar to S2, this layer performs average pooling with 2x2 windows and a\n",
    "stride of 2, resulting in 5x5x16 feature maps.\n",
    "\n",
    "C5 - Convolutional Layer: The third convolutional layer uses 120 filters (5x5 kernels), generating feature maps\n",
    "of size 1x1x120. The sigmoid activation function is applied.\n",
    "\n",
    "Flatten and Fully Connected Layers: The fully connected layers connect the output of the convolutional layers to \n",
    "the final classification layer. The first fully connected layer has 84 units with a sigmoid activation function, \n",
    "and the second fully connected layer, which serves as the output layer, has 10 units corresponding to the 10 possible\n",
    "digit classes. A softmax activation function is used for classification.\n",
    "\n",
    "LeNet-5's contributions include the concept of convolutional layers for hierarchical feature extraction, subsampling \n",
    "for spatial reduction, and the idea of stacking convolutional and fully connected layers. While modern architectures \n",
    "have evolved with deeper layers and more advanced activation functions, LeNet-5 remains a milestone in the history of\n",
    "deep learning, demonstrating the potential of CNNs for image analysis tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c04d9-c388-4b3e-ade3-2c368f985ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac5c44-a91d-4fbb-8849-58b4d1c5eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "2-Describe the key components of LeNet-5 and their respective purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a78c952-ff0d-4305-8082-0e75123089d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet-5, a foundational convolutional neural network (CNN), comprises key components that work together to\n",
    "enable effective image recognition. These components are designed to extract and transform features hierarchically, \n",
    "leading to accurate classification:\n",
    "\n",
    "Convolutional Layers (C1, C3, C5): These layers perform convolutions using learnable filters. In C1, 5x5 filters extract \n",
    "basic features like edges and textures. In C3 and C5, 5x5 filters are employed to capture more complex features. \n",
    "These layers focus on local patterns, facilitating translation invariance.\n",
    "\n",
    "Subsampling Layers (S2, S4): Subsampling, achieved through average pooling, reduces spatial dimensions while retaining\n",
    "important features. S2 and S4 layers with 2x2 windows and a stride of 2 downsample the feature maps, aiding in spatial \n",
    "hierarchy creation and reducing computation.\n",
    "\n",
    "Fully Connected Layers (F6 and Output Layer): The fully connected layers aggregate information from the previous layers.\n",
    "F6 with 84 units processes high-level abstractions. The final output layer, with 10 units and a softmax activation,\n",
    "classifies input digits. These layers ensure feature aggregation and transformation into class probabilities.\n",
    "\n",
    "Activation Functions (Sigmoid, Softmax): Sigmoid activation in convolutional and fully connected layers introduces \n",
    "non-linearity and feature extraction. Softmax in the output layer normalizes class scores to produce a probability \n",
    "distribution over classes, facilitating classification.\n",
    "\n",
    "Weight Sharing and Sparse Connections: LeNet-5's convolutional layers utilize weight sharing, reducing the number of \n",
    "learnable parameters. Sparse connections, achieved through subsampling, enforce spatial hierarchies and control overfitting.\n",
    "\n",
    "Together, these components of LeNet-5 introduce feature extraction, transformation, spatial hierarchy, and classification \n",
    "capabilities. This architecture's success paved the way for subsequent advancements in CNN design and its application to\n",
    "image recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13152b0d-55bd-4090-bd4f-8a5a7f48395a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa7d58-3d55-4ee2-993a-abd4afb26692",
   "metadata": {},
   "outputs": [],
   "source": [
    "3- Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282e9ee-36fc-4dff-9d76-9e3a6b430a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages of LeNet-5:\n",
    "\n",
    "Pioneering Architecture: LeNet-5 was a groundbreaking architecture that introduced the concept of \n",
    "convolutional layers, making it a foundational model for image classification tasks.\n",
    "\n",
    "Feature Hierarchy: The design of convolutional and subsampling layers creates a hierarchical feature\n",
    "extraction process, allowing the network to learn complex patterns from low-level to high-level features.\n",
    "\n",
    "Translation Invariance: LeNet-5's convolutional layers, followed by subsampling, enable the network to\n",
    "be less sensitive to small translations in input images, enhancing robustness.\n",
    "\n",
    "Effective on Small Images: LeNet-5 was designed for handwritten digit recognition on 32x32 images. \n",
    "Its architecture is well-suited for scenarios where images have limited spatial resolution.\n",
    "\n",
    "Limitations of LeNet-5:\n",
    "\n",
    "Limited Depth: LeNet-5 is relatively shallow compared to modern CNNs. Deeper networks often perform better on \n",
    "complex tasks as they can learn more abstract and intricate features.\n",
    "\n",
    "Limited Capacity: The limited number of parameters and units in its layers may restrict its ability to capture \n",
    "complex patterns in larger and more diverse datasets.\n",
    "\n",
    "Small Receptive Field: Due to the small filter sizes and pooling operations, LeNet-5's receptive field is limited, \n",
    "which might hinder its performance on tasks requiring global context.\n",
    "\n",
    "Overfitting Risk: LeNet-5, like early CNNs, lacks some modern regularization techniques. On large datasets, it might\n",
    "be susceptible to overfitting without additional techniques to control model complexity.\n",
    "\n",
    "In the context of its time, LeNet-5 was a remarkable achievement, serving as a foundational architecture for CNNs.\n",
    "However, its limitations in terms of depth, capacity, and modern regularization methods highlight the advancements \n",
    "made in subsequent architectures like AlexNet, VGG, and more recent models like ResNet and Inception, which address \n",
    "these limitations and demonstrate higher performance on diverse image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c911d-d86d-4823-9552-1b4b65193a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c8fd0-9999-4884-bef5-c17acabb88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTocch) and \n",
    "    train it on a publicly available dataset (e.g., MNIST). Evaluate its pecformance and provide \n",
    "insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0157a9-a9a6-4df5-892a-cd2a3e438405",
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet-5 using TensorFlow and training it on the MNIST dataset. Here's a simplified code snippet for the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d7fc131-4dc7-4bd9-bb07-fe3f0ce6e974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.8)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.4)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.33.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-2.3.7-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.28.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.11)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.57.0 keras-2.13.1 libclang-16.0.6 markdown-3.4.4 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.33.0 termcolor-2.3.0 werkzeug-2.3.7 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e8bba6-3eeb-4ad3-9a6a-472959b66952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 14:05:24.629507: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 14:05:24.723455: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 14:05:24.724783: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-28 14:05:26.937160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n",
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 63s 36ms/step - loss: 0.2089 - accuracy: 0.9359 - val_loss: 0.0773 - val_accuracy: 0.9792\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 60s 36ms/step - loss: 0.0712 - accuracy: 0.9783 - val_loss: 0.0589 - val_accuracy: 0.9828\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 60s 36ms/step - loss: 0.0523 - accuracy: 0.9834 - val_loss: 0.0540 - val_accuracy: 0.9847\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 61s 36ms/step - loss: 0.0402 - accuracy: 0.9870 - val_loss: 0.0556 - val_accuracy: 0.9832\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 64s 38ms/step - loss: 0.0330 - accuracy: 0.9895 - val_loss: 0.0524 - val_accuracy: 0.9860\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 63s 37ms/step - loss: 0.0274 - accuracy: 0.9908 - val_loss: 0.0472 - val_accuracy: 0.9870\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 60s 36ms/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.0569 - val_accuracy: 0.9850\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 61s 36ms/step - loss: 0.0204 - accuracy: 0.9934 - val_loss: 0.0528 - val_accuracy: 0.9872\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 62s 37ms/step - loss: 0.0185 - accuracy: 0.9937 - val_loss: 0.0440 - val_accuracy: 0.9893\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 61s 36ms/step - loss: 0.0149 - accuracy: 0.9949 - val_loss: 0.0522 - val_accuracy: 0.9887\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.0390 - accuracy: 0.9889\n",
      "Test accuracy: 0.9889000058174133\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Build LeNet-5 model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(16, (5, 5), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='relu'),\n",
    "    layers.Dense(84, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images.reshape(-1, 28, 28, 1), train_labels, epochs=10, validation_split=0.1)\n",
    "\n",
    "# Evaluate performance on test data\n",
    "test_loss, test_acc = model.evaluate(test_images.reshape(-1, 28, 28, 1), test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5329506c-ee90-425a-a471-ff754526ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this code, we define a LeNet-5 model using TensorFlow's Keras API. We load the MNIST dataset,\n",
    "\n",
    "preprocess the images, and build the LeNet-5 architecture with convolutional, pooling, and fully\n",
    "connected layers. We compile the model with appropriate loss and optimizer, then train it on the training data.\n",
    "\n",
    "After training, we evaluate the model's performance on the test dataset and print the test accuracy.\n",
    "LeNet-5 should achieve a reasonably high accuracy on MNIST since it was designed for handwritten digit \n",
    "recognition, achieving accuracy around 98% or more.\n",
    "\n",
    "Insights:\n",
    "LeNet-5's performance on MNIST demonstrates its suitability for simple image classification tasks.\n",
    "Its accuracy might not be as high as more modern architectures due to its limited depth and capacity. \n",
    "However, LeNet-5's accuracy could be improved with advanced techniques like data augmentation, learning \n",
    "rate schedules, and regularization.\n",
    "\n",
    "For more complex datasets, especially those with larger and varied images, deeper and more modern \n",
    "architectures would likely outperform LeNet-5 due to their enhanced ability to capture intricate features.\n",
    "\n",
    "Nevertheless, implementing and training LeNet-5 provides a practical understanding of early CNNs and their \n",
    "role in shaping modern deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e03af-a174-403a-b8b9-c7f3a0f90350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468044a9-4adc-4138-b4f9-6133142acb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC: Analyzing AlexNet.\n",
    "\n",
    "1- Present an overview of the AlexNet architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b610a23-9ba3-4095-8d49-a326680784c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet is a pioneering convolutional neural network (CNN) architecture that played a pivotal role\n",
    "in popularizing deep learning for image classification tasks. It was developed by Alex Krizhevsky, \n",
    "Ilya Sutskever, and Geoffrey Hinton, and won the ImageNet Large Scale Visual Recognition Challenge \n",
    "(ILSVRC) in 2012 with a significant margin.\n",
    "\n",
    "The AlexNet architecture consists of eight layers: five convolutional layers followed by three fully connected layers:\n",
    "\n",
    "Input Layer: Accepts RGB images of size 227x227 pixels.\n",
    "\n",
    "Convolutional Layers (C1-C5): The first layer (C1) employs 96 filters of size 11x11 with a stride of 4.\n",
    "Subsequent convolutional layers (C2-C5) use smaller filters and include max-pooling operations.\n",
    "These layers perform feature extraction and hierarchical representation learning.\n",
    "\n",
    "Fully Connected Layers (F6-F8): After flattening the output from the last convolutional layer, \n",
    "the architecture consists of three fully connected layers with 4096, 4096, and 1000 neurons, respectively.\n",
    "Dropout is applied to combat overfitting.\n",
    "\n",
    "Output Layer: The final layer has 1000 units corresponding to the number of ImageNet classes, with a softmax \n",
    "activation for classification.\n",
    "\n",
    "AlexNet employed rectified linear units (ReLUs) as activation functions, reducing the vanishing gradient problem\n",
    "and accelerating convergence. It used data augmentation, dropout, and GPU acceleration for training. AlexNet's\n",
    "deep architecture and innovative techniques laid the foundation for modern deep CNN architectures, demonstrating \n",
    "the potential of deep learning on large-scale image datasets and revolutionizing computer vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8bfaa-e914-469c-9801-a0d536557119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb31b01c-501d-4fc0-894e-5f065065293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "2- Explain the architectural innovations introduced in AlexNet that contributed to its breakthcough \n",
    "performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c965a-bf3e-401f-bf1a-938cbc9ee6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet introduced several architectural innovations that contributed to its breakthrough\n",
    "performance and played a crucial role in advancing the field of deep learning:\n",
    "\n",
    "Deep Architecture: AlexNet was significantly deeper compared to previous neural networks. \n",
    "Its depth allowed it to learn more abstract and complex features, enabling better representation \n",
    "of hierarchical patterns in images.\n",
    "\n",
    "Convolutional Layers with ReLUs: AlexNet used rectified linear units (ReLUs) as activation functions.\n",
    "ReLUs mitigated the vanishing gradient problem and enabled faster convergence during training by avoiding\n",
    "saturation issues that were common with traditional activation functions like sigmoid or tanh.\n",
    "\n",
    "Local Response Normalization (LRN): LRN was introduced to enhance the network's ability to generalize across\n",
    "variations in illumination and contrast. It acted as a form of lateral inhibition, emphasizing the responses \n",
    "of neurons that were most active relative to their neighbors, enhancing the network's discriminative power.\n",
    "\n",
    "Overlapping Max-Pooling: Instead of traditional non-overlapping pooling, AlexNet used overlapping max-pooling \n",
    "\n",
    "with stride 2. This allowed the network to capture more spatial information and increase translation invariance,\n",
    "leading to better performance.\n",
    "\n",
    "Data Augmentation and Dropout: Data augmentation techniques, such as cropping, flipping, and altering brightness,\n",
    "were employed to artificially increase the size of the training dataset. Additionally, dropout was used in fully \n",
    "connected layers to prevent overfitting, making the network more robust and generalizable.\n",
    "\n",
    "GPU Acceleration: AlexNet's training was accelerated using GPUs, which allowed for faster computations and made it\n",
    "possible to train deep networks in a feasible amount of time.\n",
    "\n",
    "Large-Scale Training: AlexNet was trained on the large-scale ImageNet dataset, containing millions of images across\n",
    "a wide range of classes. This enabled the network to learn a diverse set of features and generalize well to new,\n",
    "unseen data.\n",
    "\n",
    "These architectural innovations collectively enabled AlexNet to achieve a significantly lower error rate compared \n",
    "to previous approaches in the ImageNet competition. Its success not only demonstrated the potential of deep learning \n",
    "but also inspired subsequent advancements in CNN architectures and methodologies, shaping the landscape of modern\n",
    "computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f4f7a-b423-4ef8-bdc4-e09ffe6fbad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b6999-a7fc-4de6-ad91-715295d3d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "3- Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded9080-0400-4cdd-bd8e-4c186171097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "In AlexNet, convolutional layers, pooling layers, and fully connected layers play distinct but \n",
    "complementary roles, contributing to the architecture's exceptional performance in image classification:\n",
    "\n",
    "Convolutional Layers:\n",
    "Convolutional layers are responsible for extracting hierarchical features from input images. In AlexNet,\n",
    "the convolutional layers (C1-C5) perform feature extraction by convolving learnable filters with the input data.\n",
    "The use of multiple convolutional layers allows the network to progressively capture increasingly abstract\n",
    "and complex patterns, such as edges, textures, and shapes. Convolutional layers, aided by ReLU activations,\n",
    "introduce non-linearity and help the network learn discriminative features effectively. The deeper architecture\n",
    "of AlexNet compared to previous networks enables it to learn more intricate features, boosting its representational power.\n",
    "\n",
    "Pooling Layers:\n",
    "Pooling layers, specifically max-pooling layers, are crucial for spatial reduction and translation invariance.\n",
    "In AlexNet, max-pooling is employed after certain convolutional layers (C1, C3) and subsampling (downsampling) \n",
    "is performed. These pooling layers help in reducing the spatial dimensions of the feature maps while retaining \n",
    "essential features. Overlapping max-pooling, another innovation in AlexNet, further enhances the network's ability \n",
    "to capture spatial hierarchies and improves its robustness to translation and variation.\n",
    "\n",
    "Fully Connected Layers:\n",
    "The fully connected layers (F6-F8) in AlexNet aggregate the high-level features extracted by the previous convolutional\n",
    "and pooling layers. These layers serve as a classifier by transforming the learned features into class probabilities.\n",
    "F6 and F7 layers have a large number of neurons, capturing intricate relationships between features. Dropout, applied \n",
    "to these layers, reduces overfitting by randomly deactivating neurons during training. The final fully connected layer \n",
    "(F8) has 1000 neurons corresponding to the number of ImageNet classes and employs the softmax activation for classification.\n",
    "\n",
    "In essence, convolutional layers extract features, pooling layers reduce spatial dimensions, and fully connected layers \n",
    "perform classification in AlexNet. These components work together to enable hierarchical feature learning, translation\n",
    "invariance, and effective classification, making AlexNet a foundational model for modern deep learning architectures in \n",
    "image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b52a658-9109-4979-857f-036fdaa736a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae8af2-4e55-434b-97ea-0d473afefff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "4- Implement AlexNet using a deep learning framework of yorc choice and evaluate its pecformance \n",
    "on a dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9915d62-cda8-4147-b29e-ffbd9fc0111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Below is a simplified implementation of AlexNet using TensorFlow and evaluation on the CIFAR-10 dataset,\n",
    "which contains 60,000 32x32 color images across 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182772b-b644-4052-a1f8-7ad900e09dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets\n",
    "\n",
    "# Load and preprocess CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Build AlexNet model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((3, 3), strides=(2, 2)),\n",
    "    layers.Conv2D(256, (5, 5), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((3, 3), strides=(2, 2)),\n",
    "    layers.Conv2D(384, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(384, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((3, 3), strides=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, validation_split=0.1)\n",
    "\n",
    "# Evaluate performance on test data\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31077317-2a11-4f10-b35d-7a3c29c60dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code defines an AlexNet architecture using TensorFlow's Keras API. The model is trained on the \n",
    "CIFAR-10 dataset and evaluated on the test dataset. Note that for a comprehensive evaluation,\n",
    "more advanced preprocessing,\n",
    "hyperparameter tuning, and data augmentation techniques should be employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f67b9-518f-4f33-972c-1c72e575a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "......................................................The End......................................"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3881c32-92fc-4cf5-88b9-78c2d73853d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
